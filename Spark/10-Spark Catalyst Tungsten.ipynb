{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b126d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/07 04:08:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('trip_count_sql').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4417ed",
   "metadata": {},
   "source": [
    "CSV 를 불러와서 Spark DF 로 만들어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da682b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filepath = '/home/ubuntu/working/spark-examples/data/titanic_train.csv'\n",
    "#'hdfs://' 등 스킴만 달라짐\n",
    "\n",
    "\n",
    "titanic_sdf = spark.read.csv(filepath,\n",
    "                            inferSchema = True, # 데이터 타입을 스파크가 자동으로 인식\n",
    "                            header = True) # 첫 줄을 불러올지\n",
    "                \n",
    "titanic_sdf.show(5)                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e7d2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic_sdf.createOrReplaceTempView('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71a192d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select * \n",
    "from titanic\n",
    "limit 5\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbad94c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+----------+\n",
      "|Embarked|Pclass|male_count|\n",
      "+--------+------+----------+\n",
      "|       C|     1|        42|\n",
      "|       C|     2|        10|\n",
      "|       C|     3|        43|\n",
      "|       Q|     1|         1|\n",
      "|       Q|     2|         1|\n",
      "|       Q|     3|        39|\n",
      "|       S|     1|        79|\n",
      "|       S|     2|        97|\n",
      "|       S|     3|       265|\n",
      "+--------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 남자 중 Embarked(탑승지) 별 Pclass(좌석 등급) 마다 몇명이 탔는지\n",
    "\n",
    "query = \"\"\"\n",
    "select Embarked, Pclass, count(*) as male_count\n",
    "from titanic\n",
    "where Sex = 'male'\n",
    "group by Embarked, Pclass\n",
    "order by Embarked, Pclass\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7193d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Embarked ASC NULLS FIRST, 'Pclass ASC NULLS FIRST], true\n",
      "+- 'Aggregate ['Embarked, 'Pclass], ['Embarked, 'Pclass, 'count(1) AS male_count#202]\n",
      "   +- 'Filter ('Sex = male)\n",
      "      +- 'UnresolvedRelation [titanic], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Embarked: string, Pclass: int, male_count: bigint\n",
      "Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#27, Pclass#18], [Embarked#27, Pclass#18, count(1) AS male_count#202L]\n",
      "   +- Filter (Sex#20 = male)\n",
      "      +- SubqueryAlias titanic\n",
      "         +- View (`titanic`, [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27])\n",
      "            +- Relation [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true\n",
      "+- Aggregate [Embarked#27, Pclass#18], [Embarked#27, Pclass#18, count(1) AS male_count#202L]\n",
      "   +- Project [Pclass#18, Embarked#27]\n",
      "      +- Filter (isnotnull(Sex#20) AND (Sex#20 = male))\n",
      "         +- Relation [PassengerId#16,Survived#17,Pclass#18,Name#19,Sex#20,Age#21,SibSp#22,Parch#23,Ticket#24,Fare#25,Cabin#26,Embarked#27] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST], true, 0\n",
      "   +- Exchange rangepartitioning(Embarked#27 ASC NULLS FIRST, Pclass#18 ASC NULLS FIRST, 200), ENSURE_REQUIREMENTS, [plan_id=159]\n",
      "      +- HashAggregate(keys=[Embarked#27, Pclass#18], functions=[count(1)], output=[Embarked#27, Pclass#18, male_count#202L])\n",
      "         +- Exchange hashpartitioning(Embarked#27, Pclass#18, 200), ENSURE_REQUIREMENTS, [plan_id=156]\n",
      "            +- HashAggregate(keys=[Embarked#27, Pclass#18], functions=[partial_count(1)], output=[Embarked#27, Pclass#18, count#208L])\n",
      "               +- Project [Pclass#18, Embarked#27]\n",
      "                  +- Filter (isnotnull(Sex#20) AND (Sex#20 = male))\n",
      "                     +- FileScan csv [Pclass#18,Sex#20,Embarked#27] Batched: false, DataFilters: [isnotnull(Sex#20), (Sex#20 = male)], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/titanic_train.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Sex), EqualTo(Sex,male)], ReadSchema: struct<Pclass:int,Sex:string,Embarked:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52f8b1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7e97b4",
   "metadata": {},
   "source": [
    "택시 데이터에 대한 다음 쿼리의 실행 계획을 분석. 테이블 이름은 `mobility_data`\n",
    "\n",
    "```sql\n",
    "select \n",
    "    pickup_date, \n",
    "    count(*) as trips\n",
    "from (  select\n",
    "            split(pickup_datetime, ' ')[0] as pickup_date\n",
    "        from mobility_data )\n",
    "group by pickup_date\n",
    "````\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22f2ea97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName('texi_count_sql').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56ab2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0005|              B02510|2020-03-01 00:03:40|2020-03-01 00:23:39|          81|         159|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:28:05|2020-03-01 00:38:57|         168|         119|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:03:07|2020-03-01 00:15:04|         137|         209|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:18:42|2020-03-01 00:38:42|         209|          80|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:44:24|2020-03-01 00:58:44|         256|         226|   null|\n",
      "|           HV0003|              B02682|2020-03-01 00:17:23|2020-03-01 00:39:35|          79|         263|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:01:18|2020-03-01 00:38:52|          61|          29|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:43:27|2020-03-01 00:47:27|         150|         150|      1|\n",
      "|           HV0003|              B02764|2020-03-01 00:52:23|2020-03-01 01:00:15|         150|         210|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:19:49|2020-03-01 00:23:40|          60|         167|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:29:34|2020-03-01 00:39:19|          47|         213|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:41:44|2020-03-01 00:58:13|         213|         235|   null|\n",
      "|           HV0003|              B02765|2020-03-01 00:11:26|2020-03-01 00:24:46|         243|         153|   null|\n",
      "|           HV0003|              B02765|2020-03-01 00:28:05|2020-03-01 00:38:56|         127|          18|   null|\n",
      "|           HV0003|              B02765|2020-03-01 00:44:28|2020-03-01 00:52:09|          18|         169|   null|\n",
      "|           HV0003|              B02765|2020-03-01 00:56:50|2020-03-01 00:59:26|          94|         169|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:56:14|2020-03-01 01:03:38|         211|         158|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:14:15|2020-03-01 00:26:47|         246|         107|   null|\n",
      "|           HV0003|              B02764|2020-03-01 00:31:38|2020-03-01 00:58:07|         234|           9|   null|\n",
      "|           HV0005|              B02510|2020-03-01 00:26:31|2020-03-01 00:38:07|         139|          10|      1|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filepath = '/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv'\n",
    "\n",
    "taxi_df = spark.read.csv(filepath,\n",
    "                            inferSchema = True,\n",
    "                            header = True) \n",
    "            \n",
    "taxi_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bdc838cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_df.createOrReplaceTempView('mobility_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba54ed1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bf3e84b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:=================================================>        (6 + 1) / 7]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------+\n",
      "|pickup_date| trips|\n",
      "+-----------+------+\n",
      "| 2020-03-03|697880|\n",
      "| 2020-03-02|648986|\n",
      "| 2020-03-01|784246|\n",
      "| 2020-03-06|872012|\n",
      "| 2020-03-05|731165|\n",
      "| 2020-03-04|707879|\n",
      "| 2020-03-09|628940|\n",
      "| 2020-03-08|731222|\n",
      "| 2020-03-07|886071|\n",
      "| 2020-03-10|626474|\n",
      "| 2020-03-12|643257|\n",
      "| 2020-03-11|628601|\n",
      "| 2020-03-16|391518|\n",
      "| 2020-03-13|660914|\n",
      "| 2020-03-15|448125|\n",
      "| 2020-03-14|569397|\n",
      "| 2020-03-26|141607|\n",
      "| 2020-03-25|141088|\n",
      "| 2020-03-20|261900|\n",
      "| 2020-03-24|141686|\n",
      "+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "select \n",
    "    pickup_date, \n",
    "    count(*) as trips\n",
    "from (  select\n",
    "            split(pickup_datetime, ' ')[0] as pickup_date\n",
    "        from mobility_data )\n",
    "group by pickup_date\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1a426a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Aggregate ['pickup_date], ['pickup_date, 'count(1) AS trips#517]\n",
      "+- 'SubqueryAlias __auto_generated_subquery_name\n",
      "   +- 'Project ['split('pickup_datetime,  )[0] AS pickup_date#516]\n",
      "      +- 'UnresolvedRelation [mobility_data], [], false\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "pickup_date: string, trips: bigint\n",
      "Aggregate [pickup_date#516], [pickup_date#516, count(1) AS trips#517L]\n",
      "+- SubqueryAlias __auto_generated_subquery_name\n",
      "   +- Project [split(pickup_datetime#451,  , -1)[0] AS pickup_date#516]\n",
      "      +- SubqueryAlias mobility_data\n",
      "         +- View (`mobility_data`, [hvfhs_license_num#449,dispatching_base_num#450,pickup_datetime#451,dropoff_datetime#452,PULocationID#453,DOLocationID#454,SR_Flag#455])\n",
      "            +- Relation [hvfhs_license_num#449,dispatching_base_num#450,pickup_datetime#451,dropoff_datetime#452,PULocationID#453,DOLocationID#454,SR_Flag#455] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Aggregate [pickup_date#516], [pickup_date#516, count(1) AS trips#517L]\n",
      "+- Project [split(pickup_datetime#451,  , -1)[0] AS pickup_date#516]\n",
      "   +- Relation [hvfhs_license_num#449,dispatching_base_num#450,pickup_datetime#451,dropoff_datetime#452,PULocationID#453,DOLocationID#454,SR_Flag#455] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[pickup_date#516], functions=[count(1)], output=[pickup_date#516, trips#517L])\n",
      "   +- Exchange hashpartitioning(pickup_date#516, 200), ENSURE_REQUIREMENTS, [plan_id=421]\n",
      "      +- HashAggregate(keys=[pickup_date#516], functions=[partial_count(1)], output=[pickup_date#516, count#522L])\n",
      "         +- Project [split(pickup_datetime#451,  , -1)[0] AS pickup_date#516]\n",
      "            +- FileScan csv [pickup_datetime#451] Batched: false, DataFilters: [], Format: CSV, Location: InMemoryFileIndex(1 paths)[file:/home/ubuntu/working/spark-examples/data/fhvhv_tripdata_2020-03.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<pickup_datetime:string>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(query).explain(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "781a6049",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85fe4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
