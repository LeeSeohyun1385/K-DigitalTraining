{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57f49084",
   "metadata": {},
   "source": [
    "# 사이킷런 모델 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2341cd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting scipy>=1.3.2\n",
      "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m41.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17.3 in /home/ubuntu/.local/lib/python3.8/site-packages (from scikit-learn) (1.24.3)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 scipy-1.10.1 threadpoolctl-3.1.0\n"
     ]
    }
   ],
   "source": [
    "#!pip install sklearn\n",
    "#!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f374bcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0f38fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal_length</th>\n",
       "      <th>sepal_width</th>\n",
       "      <th>petal_length</th>\n",
       "      <th>petal_width</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal_length  sepal_width  petal_length  petal_width  target\n",
       "0             5.1          3.5           1.4          0.2       0\n",
       "1             4.9          3.0           1.4          0.2       0\n",
       "2             4.7          3.2           1.3          0.2       0\n",
       "3             4.6          3.1           1.5          0.2       0\n",
       "4             5.0          3.6           1.4          0.2       0\n",
       "..            ...          ...           ...          ...     ...\n",
       "145           6.7          3.0           5.2          2.3       2\n",
       "146           6.3          2.5           5.0          1.9       2\n",
       "147           6.5          3.0           5.2          2.0       2\n",
       "148           6.2          3.4           5.4          2.3       2\n",
       "149           5.9          3.0           5.1          1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iris datasets 로딩\n",
    "iris = load_iris()\n",
    "\n",
    "iris_data  = iris.data # feature\n",
    "iris_label = iris.target # label\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "iris_pdf = pd.DataFrame(iris_data, columns=iris_columns)\n",
    "iris_pdf['target'] = iris_label\n",
    "iris_pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b5d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_pdf.to_csv(\"./data/iris.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "500c7c3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 분할 및 모델 생성\n",
    "# from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.tree import DecisionTreeClassifier # Estimator\n",
    "from sklearn.model_selection import train_test_split # RandomSpliter\n",
    "\n",
    "X_train, X_test, t_train, t_test = train_test_split(\n",
    "    iris_data,\n",
    "    iris_label,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "tree_clf = DecisionTreeClassifier()\n",
    "tree_clf.fit(X_train, t_train) # 훈련! tree_clf 모델 자체에서 훈련이 일어나게 된다.\n",
    "\n",
    "pred = tree_clf.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf05934",
   "metadata": {},
   "source": [
    "# Spark ML 사용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c28861bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/spark-3.2.4/jars/spark-unsafe_2.12-3.2.4.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/13 04:17:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"tree-clf\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f879367",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 0:>                                                          (0 + 1) / 1]\r",
      "\r",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         5.1|        3.5|         1.4|        0.2|     0|\n",
      "|         4.9|        3.0|         1.4|        0.2|     0|\n",
      "|         4.7|        3.2|         1.3|        0.2|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "|         5.0|        3.6|         1.4|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iris_filepath = \"/home/ubuntu/working/spark-examples/data/iris.csv\"\n",
    "iris_sdf = spark.read.csv(f\"file://{iris_filepath}\", inferSchema=True, header=True)\n",
    "iris_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4bb162e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomSplit 메소드를 활용해 훈련 / 테스트 데이터 세트 분할\n",
    "train_sdf, test_sdf = iris_sdf.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fbde6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|\n",
      "|         4.6|        3.4|         1.4|        0.3|     0|\n",
      "|         4.7|        3.2|         1.3|        0.2|     0|\n",
      "|         4.7|        3.2|         1.6|        0.2|     0|\n",
      "|         4.8|        3.0|         1.4|        0.1|     0|\n",
      "|         4.8|        3.0|         1.4|        0.3|     0|\n",
      "|         4.8|        3.4|         1.6|        0.2|     0|\n",
      "|         4.8|        3.4|         1.9|        0.2|     0|\n",
      "|         4.9|        2.4|         3.3|        1.0|     1|\n",
      "|         4.9|        2.5|         4.5|        1.7|     2|\n",
      "|         4.9|        3.0|         1.4|        0.2|     0|\n",
      "|         4.9|        3.1|         1.5|        0.2|     0|\n",
      "|         4.9|        3.6|         1.4|        0.1|     0|\n",
      "|         5.0|        2.0|         3.5|        1.0|     1|\n",
      "|         5.0|        3.0|         1.6|        0.2|     0|\n",
      "|         5.0|        3.2|         1.2|        0.2|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 데이터세트는 어떻게 변환이 되어도 하나만 존재하는게 좋다\n",
    "# 모델을 여러개 사용해서 변환이 되는 상황\n",
    "# 훈련 데이터가 모델에 들어가면 transform 이 일어나게 된다.\n",
    "# 여러 번의 훈련을 거치게 되면 transform이 여러번 일어나게 된다.\n",
    "# -> train_sdsfrk 메모리 내에 여러 개가 똑같은 것이 생길 수 있다.\n",
    "\n",
    "# 훈련 직전에 사용할 데이터는 캐싱을 하는게 좋다.\n",
    "train_sdf.cache()\n",
    "train_sdf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71978cfc",
   "metadata": {},
   "source": [
    "`VectorAssembler` 를 이용하여 모든 feature 컬럼을 하나의 feature vector로 만든다. (행벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb550a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.3|        3.0|         1.1|        0.1|     0|[4.3,3.0,1.1,0.1]|\n",
      "|         4.4|        2.9|         1.4|        0.2|     0|[4.4,2.9,1.4,0.2]|\n",
      "|         4.4|        3.2|         1.3|        0.2|     0|[4.4,3.2,1.3,0.2]|\n",
      "|         4.5|        2.3|         1.3|        0.3|     0|[4.5,2.3,1.3,0.3]|\n",
      "|         4.6|        3.1|         1.5|        0.2|     0|[4.6,3.1,1.5,0.2]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# 합쳐질 컬럼 목록\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# VectorAssembler로 데이터프레임에 있는 데이터를 하나의 행벡터로 합쳐준다.\n",
    "\n",
    "vec_assembler = VectorAssembler(inputCols = iris_columns, outputCol=\"features\")\n",
    "# inputCols : 합칠 컬럼들의 목록\n",
    "# outputCol : 데이터가 어셈블된 컬럼 (합쳐진 컬럼의 이름)\n",
    "\n",
    "# VectorAssembler Transform\n",
    "train_feature_vector_sdf = vec_assembler.transform(train_sdf)\n",
    "train_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77793ae4",
   "metadata": {},
   "source": [
    "# Estimator\n",
    "Spark ML의 모델 추정기(Estimator)지만, 데이터를 변환시키는 Transformer에 해당한다.\n",
    "- train 데이터를 받아서 예측 값(prediction)으로 변환시키는 transform 과정이 일어나기 때문"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c9cce9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "# 모델 생성. 어떤 컬럼의 데이터를 이용해서 학습할지 결정을 지어줘야 한다.\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol = \"target\",\n",
    "    maxDepth=5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "106666bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassifier"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dt) # 알고리즘 껍데기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58923591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.classification.DecisionTreeClassificationModel"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 학습. fit()메소드를 이용하여 학습을 수행하고, 그 결과를 ML 모델로 변환한다.\n",
    "dt_model = dt.fit(train_feature_vector_sdf)\n",
    "type(dt_model) # 훈련이 끝나면 모델이 생성됨을 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17510922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|\n",
      "+------------+-----------+------------+-----------+------+\n",
      "only showing top 5 rows\n",
      "\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|\n",
      "+------------+-----------+------------+-----------+------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터 예측\n",
    "test_sdf.show(5)\n",
    "\n",
    "# 훈련 데이터에서 적용시켰던 Transformer를 테스트 세트에다가도 그대로 적용시킨다.\n",
    "# 테스트 세트를 위한 새로운 Transformer를 만들지 않는다.\n",
    "test_feature_vector_sdf = vec_assembler.transform(test_sdf)\n",
    "test_feature_vector_sdf.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e30e5950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = dt_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab24860d",
   "metadata": {},
   "source": [
    "# 모델 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "556b5701",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol = \"target\",\n",
    "    predictionCol = \"prediction\",\n",
    "    metricName = \"accuracy\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c3120832",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/13 05:17:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "23/06/13 05:17:28 WARN InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features|       rawPrediction|         probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[18.6086266693526...|[0.99997762791224...|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[18.8180066107263...|[0.99997581287298...|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[22.6963845270307...|[0.99999942608846...|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[16.7506644665745...|[0.99971232954776...|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[17.3393987944099...|[0.99969323067671...|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "정확도 1.0\n"
     ]
    }
   ],
   "source": [
    "# LogisticRegression 사용해 보기 [실습]\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "# ML 알고리즘 객체 생성\n",
    "lr = LogisticRegression(\n",
    "    featuresCol = \"features\",\n",
    "    labelCol = \"target\",\n",
    "    maxIter = 10 # 경사하강법 시행 횟수\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_feature_vector_sdf)\n",
    "\n",
    "predictions = lr_model.transform(test_feature_vector_sdf)\n",
    "predictions.show(5)\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "print(\"정확도\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a14503c",
   "metadata": {},
   "source": [
    "# 파이프라인 구축\n",
    "- pipeline은 여러 개의 개별적인 Transformer의 변환 작업, Estimator의 학습작업을 일련의 프로세스 연결을 통해 간단한 API 처리로 구현할 수 있게 해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2f773f22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.Pipeline"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "iris_columns = [\"sepal_length\", \"sepal_width\", \"petal_length\", \"petal_width\"]\n",
    "\n",
    "# Pipeline은 개별 변환 및 모델 학습 작업을 각각의 stage로 정의해서 파이프라인에 순서대로 등록\n",
    "# pipeline.fit() 메소드를 활용해서 순서대로 연결된 스테이지 작업을 일괄적으로 수행\n",
    "# pipeline.fit()의 결과물은 PipelineModel로 반환이 된다.\n",
    "# PipelineModel에서 예측 작업을 transform()으로 수행\n",
    "\n",
    "# 첫 번째 stage는 Feature Vectorization을 위한 VectorAssembler\n",
    "stage_1 = VectorAssembler(inputCols=iris_columns, outputCol=\"features\")\n",
    "\n",
    "# 두 번째 stage는 학습을 위한 모델을 생성\n",
    "stage_2 = DecisionTreeClassifier(featuresCol=\"features\", labelCol=\"target\", maxDepth=3)\n",
    "\n",
    "# 리스트를 활용해 stage를 순서대로 배치\n",
    "stages = [stage_1, stage_2]\n",
    "\n",
    "# 파이프라인에 등록\n",
    "pipeline = Pipeline(stages=stages)\n",
    "type(pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b52537d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.ml.pipeline.PipelineModel"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_model = pipeline.fit(train_sdf)\n",
    "type(pipeline_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6495686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|sepal_length|sepal_width|petal_length|petal_width|target|         features| rawPrediction|  probability|prediction|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "|         4.4|        3.0|         1.3|        0.2|     0|[4.4,3.0,1.3,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.2|         1.4|        0.2|     0|[4.6,3.2,1.4,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.6|        3.6|         1.0|        0.2|     0|[4.6,3.6,1.0,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.8|        3.1|         1.6|        0.2|     0|[4.8,3.1,1.6,0.2]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "|         4.9|        3.1|         1.5|        0.1|     0|[4.9,3.1,1.5,0.1]|[39.0,0.0,0.0]|[1.0,0.0,0.0]|       0.0|\n",
      "+------------+-----------+------------+-----------+------+-----------------+--------------+-------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 파이프라인을 통해서 테스트 세트 예측\n",
    "predictions = pipeline_model.transform(test_sdf)\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0b3bb704",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
